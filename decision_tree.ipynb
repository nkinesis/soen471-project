{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "df545e50",
   "metadata": {},
   "source": [
    "### Decision Tree Classification Model\n",
    "\n",
    "A decision tree model is a machine learning algorithm that can be used for classification tasks.\n",
    "<br>This classification model is easy to interpret and can handle both categorical and numerical features, thus explaining our choice for this algorithm. \n",
    "<br>The model represents a series of decisions that lead to a final classification decision as a tree-like graph with several nodes. \n",
    "<br>At each node of the tree, a decision is made based on a specific attribute, and each branch represents a different decision or outcome. \n",
    "<br>\n",
    "<br>In classification, the goal is to predict a categorical variable based on a set of input features.\n",
    "<br>For our case - predicting the cost of damage for a fire incident - the set of input features is as follows:\n",
    "<li><b>DateOfCall</b>: the month of the date when the fire incident was reported \n",
    "<li><b>PropertyType</b>: the type of location where the fire incident occured\n",
    "<li><b>NumPumpsAttending</b>: the number of total fire pumps that were deployed to the fire incident location\n",
    "<li><b>PumpHoursRoundUp</b>: the number of hours the fire pumps were used during the fire incident\n",
    "<li><b>mean_temp</b>: mean daily temperature in Celsius (Cº)\n",
    "<br>\n",
    "<br>\n",
    "The output of our classification model is the cost of damage in pound sterling (£).\n",
    "<br>The cost value was originally a continuous numerical variable, but we converted it to a categorical variable, dividing and categorizing the numerical value in intervals of £100. \n",
    "<br>For example, all records of cost between £0.00 and £100.00 fall under category 1, all records of cost between £100.01 and £200.00 fall under category 2, and so on. All records with costs larger than £1000.01 fall under category 11. \n",
    "<br>\n",
    "<br>\n",
    "\n",
    "#### Scikit-Learn Library\n",
    "\n",
    "Our prediction model using the decision tree classifier is implemented with <b><i>scikit-learn</i></b> machine learning library in Python.<br>\n",
    "Please follow the <b>scikit-learn</b>'s installation guide ([https://scikit-learn.org/stable/install.html](Hidden_landing_URL)) and have the library ready before running the code.<br>\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5e49967b",
   "metadata": {},
   "source": [
    "#### 1. Importing Libraries\n",
    "\n",
    "Following libraries and function are necessary to implement the decision tree prediction model.\n",
    "<br>\n",
    "<br>\n",
    "<b> Pandas</b>: \n",
    "<li> Data manipulation library\n",
    "<br><br>\n",
    "<b> Numpy</b>: \n",
    "<li> Data manipulation library\n",
    "<br><br>\n",
    "<b> train_test_split </b> from sklearn.model_selection:\n",
    "<li> Dividing the data into training and testing sets for model training and peformance analysis\n",
    "<br><br>\n",
    "<b> tree </b> from sklearn:\n",
    "<li> Scikit-learn's decision tree classifier model library\n",
    "<br><br>\n",
    "<b> classification_report</b> from sklearn.metrics:\n",
    "<li> Visualizing and measure the performance of the prediction model\n",
    "<br><br>\n",
    "<b> GridSearchCV</b> from sklearn.model_selection:\n",
    "<li> Hyper parameter tuning\n",
    "<br><br>\n",
    "<b>pickle</b>:\n",
    "<li> Saving and loading machine learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c07b9daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import all necessary libraries for decision tree model training and testing\n",
    "\n",
    "#libraries for the data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#library for decision tree model\n",
    "from sklearn import tree\n",
    "\n",
    "#library for measuring the performance metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#library for hyper parameter tuning\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#library for saving the final classifier model\n",
    "import pickle"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3d4e1fb7",
   "metadata": {},
   "source": [
    "#### 2. Load Dataset\n",
    "\n",
    "Loading the fire incident and weather dataset into <i>pandas</i> dataframe from CSV file.<br>\n",
    "The features present in the cleaned dataset are listed below, alongside the type of data each of them holds.\n",
    "<br>Only selected features will be used for the prediction model.\n",
    "<br>Please refer to below for each data types' equivalent in Python.\n",
    "<br><br>\n",
    "Pandas' datatypes and their Python equivalents:\n",
    "<li> int64 = int\n",
    "<li> float64 = float\n",
    "<li> object = string\n",
    "<br><br>*Please refer to the <b><i>preprocessing</i></b> folder for detailed implementation on data cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60aa0437",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DateOfCall             int64\n",
       "CalYear                int64\n",
       "HourOfCall             int64\n",
       "IncidentGroup          int64\n",
       "PropertyCategory       int64\n",
       "PropertyType           int64\n",
       "NumPumpsAttending      int64\n",
       "PumpHoursRoundUp       int64\n",
       "Notional Cost (£)      int64\n",
       "Date                  object\n",
       "CostCat                int64\n",
       "cloud_cover          float64\n",
       "sunshine             float64\n",
       "global_radiation     float64\n",
       "max_temp             float64\n",
       "mean_temp            float64\n",
       "min_temp             float64\n",
       "precipitation        float64\n",
       "pressure             float64\n",
       "snow_depth           float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load dataset, print types\n",
    "df = pd.read_csv('preprocessing/data/london_clean_weather.csv')\n",
    "df.dtypes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9537f34b",
   "metadata": {},
   "source": [
    "#### 3. Feature Selection and Dataset Split\n",
    "\n",
    "First, the entire dataset is split into input and output features - X and y respectively.\n",
    "<br>We wish to predict feature in <i>y</i> based on features grouped in <i>X</i> used as input values.\n",
    "<br>\n",
    "<Br>\n",
    "Both groups of input and output features are split into two subsets for their respective use with the help of Scikit-Learn's <i>train_test_split</i> function:\n",
    "<li>67% of the dataset is used to train the decision tree model\n",
    "<li>33% of the dataset is used to test the performance of the decision tree model\n",
    "<br>\n",
    "<br>\n",
    "In <i>train_test_split</i> function, the <i>random_state</i> variable is specified to 42.\n",
    "<br>This specification allows <i>train_test_split</i> function to generate the identical training and testing subsets every time it is called.\n",
    "<br>This functionality allows all three prediction models to be trained on the same dataset, allowing better comparison between thier performance in the later step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2705cd45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         DateOfCall  PropertyType  NumPumpsAttending  PumpHoursRoundUp  \\\n",
      "706975            5            12                  1                 1   \n",
      "450954            9             6                  2                 1   \n",
      "525760            6            12                  2                 1   \n",
      "20577             3            83                  2                 1   \n",
      "1069644          11            37                  1                 1   \n",
      "\n",
      "         mean_temp  \n",
      "706975        15.0  \n",
      "450954        13.3  \n",
      "525760        16.4  \n",
      "20577          7.6  \n",
      "1069644        8.3  \n",
      "         CostCat\n",
      "706975         3\n",
      "450954         3\n",
      "525760         3\n",
      "20577          3\n",
      "1069644        4\n"
     ]
    }
   ],
   "source": [
    "# do train and test split\n",
    "X = df[['DateOfCall', 'PropertyType', 'NumPumpsAttending', 'PumpHoursRoundUp', 'mean_temp']]\n",
    "y = df[['CostCat']]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "# print a small sample of train X and y\n",
    "print(X_train[0:5])\n",
    "print(y_train[0:5])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "54fe14f0",
   "metadata": {},
   "source": [
    "#### 4. Model implementation and training\n",
    "Scikit-Learn's deicision tree classifier offers a range of parameters that can be tuned to control the behaviour of the decision tree model.\n",
    "<br>\n",
    "The following are the parameters we are interested in:\n",
    "<ol>\n",
    "<li><b><i>max_depth</i></b>: \n",
    "<br> - Sets the maximum depth of the decision tree\n",
    "<br> - A deeper tree can capture more complex relationships in the data, but can also lead to overfitting\n",
    "<br> - Default value = \"None\"\n",
    "<li><b><i>min_samples_split</i></b>: \n",
    "<br> - Sets the minimum number of samples required to split an internal node. \n",
    "<br> - A higher value can prevent the tree from splitting too early, leading to more robust models.\n",
    "<br> - Default value = 2\n",
    "<li><b><i>criterion</i></b>: \n",
    "<br> - Specifies the function used to measure the quality of a split. \n",
    "<br> - The two options are \"gini\" and \"entropy\", which correspond to the Gini impurity and information gain criteria, respectively.\n",
    "<br> - Default value = \"gini\"\n",
    "</ol>\n",
    "\n",
    "\n",
    "The first training and testing of the decision tree model is done with default parameters set by the Scikit-Learn library.\n",
    "<br>\n",
    "These hyper parameters will be specified in the following step, once the decision tree model provides an acceptable performance score with the current training and testing datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7abac23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and train decision tree\n",
    "# clf = tree.DecisionTreeRegressor()\n",
    "clf = tree.DecisionTreeClassifier(random_state=0)\n",
    "clf = clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09e9cec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default Decision Tree Model Description\n",
      "Depth: 52\n",
      "Number of Leaves: 106173\n",
      "*******************************************\n",
      "Classification Report of the default model\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           3       0.78      0.85      0.81    231390\n",
      "           4       0.71      0.61      0.66    140418\n",
      "           6       0.75      0.79      0.77     20572\n",
      "           7       0.66      0.60      0.63     13872\n",
      "           8       0.58      0.61      0.60      3444\n",
      "           9       0.43      0.43      0.43      1977\n",
      "          10       0.46      0.43      0.45      2228\n",
      "          11       0.93      0.93      0.93     10683\n",
      "\n",
      "    accuracy                           0.75    424584\n",
      "   macro avg       0.66      0.66      0.66    424584\n",
      "weighted avg       0.75      0.75      0.75    424584\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "y_pred = clf.predict(X_test).round()\n",
    "print(\"Default Decision Tree Model Description\")\n",
    "print(\"Depth: %d\" % clf.get_depth())\n",
    "print(\"Number of Leaves: %d\" % clf.get_n_leaves())\n",
    "print(\"*******************************************\")\n",
    "print(\"Classification Report of the default model\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2fce405d",
   "metadata": {},
   "source": [
    "#### 5. Hyper-parameter Tuning\n",
    "\n",
    "Now that the decision tree model has proven to produce acceptable performance, we will try to specify its hyper parameters to determine the combination that maxmizes the model's performance score.\n",
    "\n",
    "Scikit-Learn offers an exhuastive search algorithm enabling us to easily determine the best hyper parameter combination called <b><i>GridSearchCV</i></b>\n",
    "<br>The algorithm iterate over every combination possible of the input hyper parameter values and select the best one.\n",
    "\n",
    "The following are the values of hyperparameters tested:\n",
    "<ol>\n",
    "<li><b><i>max_depth</i></b>: \n",
    "<br> - between 45 and 55\n",
    "<li><b><i>min_samples_split</i></b>: \n",
    "<br> - range from 2 to 10: (2, 3, 4, 5, 6, 7, 8, 9, 10)\n",
    "<br> - range from 10 to 100: (10, 20, 30, 40, 50, 60, 70, 80, 90, 100)\n",
    "<li><b><i>criterion</i></b>: \n",
    "<br> - \"gini\" and \"entropy\"\n",
    "</ol>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "597eeed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters for decision tree classification:  {'criterion': 'gini', 'max_depth': 46, 'min_samples_split': 90}\n"
     ]
    }
   ],
   "source": [
    "#specify the value of parameters of interest\n",
    "tree_params = {'criterion':['gini','entropy'],'max_depth': list(range(45, 56)),'min_samples_split':list(range(2, 10))+np.arange(10, 100, 10).tolist()}\n",
    "\n",
    "#determine the best hyper parameter\n",
    "dtc_top = GridSearchCV(tree.DecisionTreeClassifier(), tree_params, cv=5)\n",
    "\n",
    "# Training the model for classification with the same dataset \n",
    "model2 = dtc_top.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best hyperparameters for decision tree classification: \", dtc_top.best_params_)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1f36d286",
   "metadata": {},
   "source": [
    "#### 6. Best Performing Model\n",
    "\n",
    "The following are the selected hyperparameter values:\n",
    "<ol>\n",
    "<li><b><i>max_depth</i></b>: 46\n",
    "<li><b><i>min_samples_split</i></b>: 90\n",
    "<li><b><i>criterion</i></b>: gini\n",
    "</ol>\n",
    "\n",
    "With the best hyperparameter combination found in the previous step, we can now build our final decision tree classifier and measure its performance.\n",
    "<br>Using the <i>pickle</i> library, the model with the best hyperparameter is saved to the project file for future use.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d9d4e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the model with the best performing hyperparameters\n",
    "filename = \"models/DT_model.pickle\"\n",
    "\n",
    "#train the model with the best hyperparameters\n",
    "model_final = tree.DecisionTreeClassifier(criterion = \"gini\", max_depth = 46, min_samples_split = 90)\n",
    "model_final.fit(X_train, y_train)\n",
    "\n",
    "# save model\n",
    "pickle.dump(model_final, open(filename, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05f4b14e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report of the default model\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           3       0.79      0.85      0.82    231390\n",
      "           4       0.71      0.62      0.66    140418\n",
      "           6       0.73      0.83      0.77     20572\n",
      "           7       0.68      0.53      0.60     13872\n",
      "           8       0.53      0.73      0.61      3444\n",
      "           9       0.42      0.31      0.36      1977\n",
      "          10       0.43      0.37      0.40      2228\n",
      "          11       0.95      0.91      0.93     10683\n",
      "\n",
      "    accuracy                           0.76    424584\n",
      "   macro avg       0.65      0.64      0.64    424584\n",
      "weighted avg       0.75      0.76      0.75    424584\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Measure performance of the model with the best hyper parameter \n",
    "y_top_pred = model_final.predict(X_test)\n",
    "print(\"Classification Report of the default model\")\n",
    "print(classification_report(y_test, y_top_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
