{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classification Model\n",
    "\n",
    "A random forest model is a type of machine learning model that is used for both classification and regression tasks. \n",
    "<br>It is an ensemble model, meaning that it is made up of a collection of decision trees.\n",
    "<br>multiple decision trees are created, each with a random subset of the available features and data points. \n",
    "<br>The trees are then combined to produce a final prediction. \n",
    "<br>\n",
    "<br>In classification, the goal is to predict a categorical variable based on a set of input features.\n",
    "<br>For our case - predicting the cost of damage for a fire incident - the set of input features is as follows:\n",
    "<li><b>DateOfCall</b>: the month of the date when the fire incident was reported \n",
    "<li><b>PropertyType</b>: the type of location where the fire incident occured\n",
    "<li><b>NumPumpsAttending</b>: the number of total fire pumps that were deployed to the fire incident location\n",
    "<li><b>PumpHoursRoundUp</b>: the number of hours the fire pumps were used during the fire incident\n",
    "<li><b>mean_temp</b>: mean daily temperature in Celsius (Cº)\n",
    "<br>\n",
    "<br>\n",
    "The output of our classification model is the cost of damage in pound sterling (£).\n",
    "<br>The cost value was originally a continuous numerical variable, but we converted it to a categorical variable, dividing and categorizing the numerical value in intervals of £100. \n",
    "<br>For example, all records of cost between £0.00 and £100.00 fall under category 1, all records of cost between £100.01 and £200.00 fall under category 2, and so on. All records with costs larger than £1000.01 fall under category 11. \n",
    "<br>\n",
    "<br>\n",
    "\n",
    "#### Scikit-Learn Library\n",
    "\n",
    "Our prediction model using the decision tree classifier is implemented with <b><i>scikit-learn</i></b> machine learning library in Python.<br>\n",
    "Please follow the <b>scikit-learn</b>'s installation guide ([https://scikit-learn.org/stable/install.html](Hidden_landing_URL)) and have the library ready before running the code.<br>\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Importing Libraries\n",
    "\n",
    "Following libraries and function are necessary to implement the decision tree prediction model.\n",
    "<br>\n",
    "<br>\n",
    "<b> Pandas</b>: \n",
    "<li> Data manipulation library\n",
    "<br><br>\n",
    "<b> Numpy</b>: \n",
    "<li> Data manipulation library\n",
    "<br><br>\n",
    "<b> train_test_split </b> from sklearn.model_selection:\n",
    "<li> Dividing the data into training and testing sets for model training and peformance analysis\n",
    "<br><br>\n",
    "<b> RandomForestClassifier </b> from sklearn:\n",
    "<li> Scikit-learn's random forest classifier model library\n",
    "<br><br>\n",
    "<b> classification_report</b> from sklearn.metrics:\n",
    "<li> Visualizing and measure the performance of the prediction model\n",
    "<br><br>\n",
    "<b> GridSearchCV</b> from sklearn.model_selection:\n",
    "<li> Hyper parameter tuning\n",
    "<br><br>\n",
    "<b>pickle</b>:\n",
    "<li> Saving and loading machine learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import all necessary libraries for random forest model training and testing\n",
    "\n",
    "#libraries for the data manipulation\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# library for data vizualization \n",
    "from matplotlib import pyplot as plt\n",
    "# used for matplotlib in jupyter notebooks\n",
    "%matplotlib inline \n",
    "import seaborn as sns\n",
    "\n",
    "#library for random forest model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# library for measuring the performance metrics\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, r2_score, classification_report\n",
    "\n",
    "#for hyper parameter tunning\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#library for saving the final classifier model\n",
    "import pickle\n",
    "\n",
    "# To change scientific numbers to float\n",
    "np.set_printoptions(formatter={'float_kind':'{:f}'.format})\n",
    "# Increases the size of sns plots\n",
    "sns.set(rc={'figure.figsize':(8,6)})\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "# from sklearn import tree\n",
    "# from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "# Datetime lib\n",
    "from pandas import to_datetime\n",
    "import itertools\n",
    "import datetime"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Load Dataset\n",
    "\n",
    "Loading the fire incident and weather dataset into <i>pandas</i> dataframe from CSV file.<br>\n",
    "The features present in the cleaned dataset are listed below, alongside the type of data each of them holds.\n",
    "<br>Only selected features will be used for the prediction model.\n",
    "<br>Please refer to below for each data types' equivalent in Python.\n",
    "<br><br>\n",
    "Pandas' datatypes and their Python equivalents:\n",
    "<li> int64 = int\n",
    "<li> float64 = float\n",
    "<li> object = string\n",
    "<br><br>*Please refer to the <b><i>preprocessing</i></b> folder for detailed implementation on data cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DateOfCall             int64\n",
       "CalYear                int64\n",
       "HourOfCall             int64\n",
       "IncidentGroup          int64\n",
       "PropertyCategory       int64\n",
       "PropertyType           int64\n",
       "NumPumpsAttending      int64\n",
       "PumpHoursRoundUp       int64\n",
       "Notional Cost (£)      int64\n",
       "Date                  object\n",
       "cloud_cover          float64\n",
       "sunshine             float64\n",
       "global_radiation     float64\n",
       "max_temp             float64\n",
       "mean_temp            float64\n",
       "min_temp             float64\n",
       "precipitation        float64\n",
       "pressure             float64\n",
       "snow_depth           float64\n",
       "CostCat                int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('preprocessing/data/london_clean.csv')\n",
    "data.dtypes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Feature Selection and Dataset Split\n",
    "\n",
    "First, the entire dataset is split into input and output features - X and y respectively.\n",
    "<br>We wish to predict feature in <i>y</i> based on features grouped in <i>X</i> used as input values.\n",
    "<br>\n",
    "<Br>\n",
    "Both groups of input and output features are split into two subsets for their respective use with the help of Scikit-Learn's <i>train_test_split</i> function:\n",
    "<li>67% of the dataset is used to train the random forest model\n",
    "<li>33% of the dataset is used to test the performance of the random forest model\n",
    "<br>\n",
    "<br>\n",
    "In <i>train_test_split</i> function, the <i>random_state</i> variable is specified to 42.\n",
    "<br>This specification allows <i>train_test_split</i> function to generate the identical training and testing subsets every time it is called.\n",
    "<br>This functionality allows all three prediction models to be trained on the same dataset, allowing better comparison between thier performance in the later step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (1286617, 5)\n",
      "y shape: (1286617, 1)\n"
     ]
    }
   ],
   "source": [
    "X = data[['DateOfCall', 'PropertyType', 'NumPumpsAttending', 'PumpHoursRoundUp', 'mean_temp']]\n",
    "y = data[['CostCat']]\n",
    "\n",
    "print('X shape: {}'.format(np.shape(X)))\n",
    "print('y shape: {}'.format(np.shape(y)))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Model implementation and training\n",
    "Scikit-Learn's Random Forest classifier offers a range of parameters that can be tuned to control the behaviour of the decision tree model.\n",
    "<br>\n",
    "The following are the parameters we are interested in:\n",
    "<ol>\n",
    "<li><b><i>n_estimators</i></b>: \n",
    "<br> - The number of trees in the forest.\n",
    "<br> - Builds multiple decision trees on different sub-samples of the dataset and averages their predictions to obtain a more stable and accurate prediction.\n",
    "<br> - Default value = 100\n",
    "<li><b><i>criterion</i></b>: \n",
    "<br> - Specifies the function used to measure the quality of a split. \n",
    "<br> - The three options are \"gini\", \"entropy\" and \"log_loss\", which correspond to the Gini impurity and information gain criteria, respectively.\n",
    "<br> - Default value = \"gini\"\n",
    "<li><b><i>max_depth</i></b>: \n",
    "<br> - Sets the maximum depth of the decision tree\n",
    "<br> - A deeper tree can capture more complex relationships in the data, but can also lead to overfitting\n",
    "<br> - Default value = \"None\"\n",
    "<li><b><i>max_features</i></b>: \n",
    "<br> - The number of features to consider when looking for the best split. \n",
    "<br> - Helps prevent overfitting and improves the generalization of the model.\n",
    "<br> - Default value = \"sqrt\"\n",
    "\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy is:  0.839866919247871\n",
      "Testing Accuracy is:  0.7713079155125958\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.83      0.80    231390\n",
      "           1       0.69      0.62      0.65    140418\n",
      "           2       1.00      1.00      1.00     34444\n",
      "           3       0.70      0.78      0.74      5421\n",
      "           4       0.58      0.51      0.54      4962\n",
      "           5       0.92      0.92      0.92      7949\n",
      "\n",
      "    accuracy                           0.77    424584\n",
      "   macro avg       0.78      0.78      0.78    424584\n",
      "weighted avg       0.77      0.77      0.77    424584\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators=10, criterion='entropy')\n",
    "rf.fit(X_train, y_train)\n",
    "prediction_test = rf.predict(X=X_test)\n",
    "\n",
    "# source: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "\n",
    "# Accuracy on Test\n",
    "print(\"Training Accuracy is: \", rf.score(X_train, y_train))\n",
    "print(\"Testing Accuracy is: \", rf.score(X_test, y_test))\n",
    "print(classification_report(y_test, prediction_test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Hyper-parameter Tuning\n",
    "\n",
    "Now that the random forest model has proven to produce acceptable performance, we will try to specify its hyper parameters to determine the combination that maxmizes the model's performance score.\n",
    "\n",
    "Scikit-Learn offers an exhuastive search algorithm enabling us to easily determine the best hyper parameter combination called <b><i>GridSearchCV</i></b>\n",
    "<br>The algorithm iterate over every combination possible of the input hyper parameter values and select the best one.\n",
    "\n",
    "The following are the values of hyperparameters tested:\n",
    "<ol>\n",
    "<li><b><i>n_estimators</i></b>: \n",
    "<br> - [5, 10]\n",
    "<li><b><i>criterion</i></b>: \n",
    "<br> - \"gini\", \"entropy\" and \"log_loss\"\n",
    "<li><b><i>max_depth</i></b>: \n",
    "<br> - [5, 7, 10]\n",
    "<li><b><i>max_features</i></b>: \n",
    "<br> - \"sqrt\", \"log2\" and None\n",
    "</ol>\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters for random forest classification:  {'criterion': 'log_loss', 'max_depth': 10, 'max_features': None, 'n_estimators': 3}\n"
     ]
    }
   ],
   "source": [
    "tree_params = {'n_estimators':[2, 3, 5],\n",
    "               'criterion':['gini','entropy', 'log_loss'],\n",
    "               'max_depth':[5, 7, 10], \n",
    "               'max_features':['sqrt', 'log2', None]}\n",
    "rf_top = GridSearchCV( RandomForestClassifier(), tree_params, cv=5)\n",
    "\n",
    "# Training the model with each combination\n",
    "rf_top2 = rf_top.fit(X_train, y_train)\n",
    "\n",
    "# Display the best hyperparameters\n",
    "print(\"Best hyperparameters for random forest classification: \", rf_top.best_params_)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Best Performing Model\n",
    "\n",
    "The following are the selected hyperparameter values:\n",
    "<ol>\n",
    "<li><b><i>criterion</i></b>: gini\n",
    "<li><b><i>max_depth</i></b>: 10\n",
    "<li><b><i>max_features</i></b>: None\n",
    "<li><b><i>n_estimators</i></b>: 10\n",
    "</ol>\n",
    "\n",
    "With the best hyperparameter combination found in the previous step, we can now build our final random forest classifier and measure its performance.\n",
    "<br>Using the <i>pickle</i> library, the model with the best hyperparameter is saved to the project file for future use.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the model with the best performing hyperparameters\n",
    "filename = \"models/RF_model.pickle\"\n",
    "\n",
    "#train the model with the best hyperparameters\n",
    "model_final = RandomForestClassifier(criterion = \"log_loss\", max_depth = 10, max_features =  None, n_estimators = 3)\n",
    "model_final.fit(X_train, y_train)\n",
    "\n",
    "# save model\n",
    "pickle.dump(model_final, open(filename, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report of the default model\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.97      0.78    231390\n",
      "           1       0.73      0.14      0.23    140418\n",
      "           2       1.00      1.00      1.00     34444\n",
      "           3       0.66      0.94      0.77      5421\n",
      "           4       0.68      0.27      0.39      4962\n",
      "           5       0.89      0.96      0.92      7949\n",
      "\n",
      "    accuracy                           0.69    424584\n",
      "   macro avg       0.77      0.71      0.68    424584\n",
      "weighted avg       0.71      0.69      0.61    424584\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Measure performance of the model with the best hyper parameter \n",
    "y_top_pred = model_final.predict(X_test)\n",
    "print(\"Classification Report of the default model\")\n",
    "print(classification_report(y_test, y_top_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Cross Validation\n",
    "While GridSearchCV already ran cross-validation, we ran it again testing different folds to make sure the accuracy results are not being influenced by a given split of the data. We observed that the accuracy of the model remains close to the values we obtained with 5-fold GridSearchCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with 8 fold:\n",
      "Accuracy: 0.670 (0.002)\n",
      "Testing with 10 fold:\n",
      "Accuracy: 0.669 (0.002)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "\n",
    "folds = [8, 10]\n",
    "for i in folds:\n",
    "    cross_val = KFold(n_splits=i, random_state=42, shuffle=True)\n",
    "    scores = cross_val_score(rf_top, X, y, scoring='accuracy', cv=cross_val, n_jobs=4)\n",
    "    print(\"Testing with {} fold:\".format(i))\n",
    "    print('Accuracy: %.3f (%.3f)' % (mean(scores), std(scores)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
